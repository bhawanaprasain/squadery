{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f1ce36",
   "metadata": {},
   "source": [
    "## Text Segmentation\n",
    "\n",
    "\n",
    "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. When segmentation bounndaries are well defined, text segmentation is simple. But in unstructured data there are no distinct boundaries and segmentation becomes tedious task.\n",
    "<br><br>\n",
    "In this notebook we will be extracting different segments like abstract, methodology, conclusion in a research paper. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98793264",
   "metadata": {},
   "source": [
    "\n",
    "## Reading a pdf file\n",
    "\n",
    "We need preprocessed text before we start segmentation, so we will be extracting text from pdf file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3534249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d841794",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='researchpaper.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3178cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdfFileObject = open(file_path, 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "count = pdfReader.numPages\n",
    "text = ''\n",
    "for i in range(count):\n",
    "    page = pdfReader.getPage(i)\n",
    "    text += page.extract_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d79d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "019fa146",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantext= text_cleaning(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee11b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "headings =['Introduction','Abstract', 'Methodology', 'Conclusion','References',\"Proposed System\",\"Conclusion and Future Work\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071e0c9",
   "metadata": {},
   "source": [
    "We need to find how many times a certain heading occurs in a given scientific paper. If it occurs muliple times then we need to find the actual heading start point. If **introduction** appears more than one time then its occurance can be mentioned in other segment of paper like abstract, methodology and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84671f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Introduction', 1),\n",
       " ('Abstract', 1),\n",
       " ('Methodology', 0),\n",
       " ('Conclusion', 1),\n",
       " ('References', 1),\n",
       " ('Proposed System', 3),\n",
       " ('Conclusion and Future Work', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading_occurance = [(heading,cleantext.count(heading.lower()) )for heading in headings]\n",
    "heading_occurance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e8418",
   "metadata": {},
   "source": [
    "`find_occurance` function returns indices where a given heading is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b958999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurance(heading):\n",
    "    occurance_indices = [word.start() for word in re.finditer(heading, cleantext)]\n",
    "    return occurance_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb8e72",
   "metadata": {},
   "source": [
    "In order to determine if a given portion of text is start of segment, we need to analyze fews words after the occurance of heading.\n",
    "If there is no new line or if there is presence of punctuation marks like `.?,;` after the heading term or some words , then it can't be the start of text. It should be the only word or phrase  in a  line, to get identified as a heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b39e62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_next_word(index,heading):\n",
    "    return cleantext[index:index+len(heading)+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b9dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_topics(headings):\n",
    "    available_headings =[]\n",
    "    headings = [heading.lower()for heading in headings]\n",
    "    start_indices = []\n",
    "    punctuations= string.punctuation \n",
    "    \n",
    "    for heading in headings:\n",
    "        frequency = cleantext.count(heading.lower())\n",
    "        occurance_indices = find_occurance(heading)\n",
    "        if len(occurance_indices) >0:\n",
    "            for index in occurance_indices:\n",
    "                next_words_seq = prob_next_word(index,heading)\n",
    "                immediate_next_word = next_words_seq[len(heading):]\n",
    "\n",
    "                length= len(immediate_next_word.replace(\" \",\"\"))\n",
    "                is_start = (\"\\n\" in next_words_seq) and (punctuations not in next_words_seq)\n",
    "                if is_start:\n",
    "                    start_indices.append(index)\n",
    "                    available_headings.append(heading)\n",
    "    return available_headings,start_indices\n",
    "available_headings,start_indices= available_topics(headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89726d",
   "metadata": {},
   "source": [
    "Start of one segment is end for another segment. Inorder to find the ending of a segment, we choose segment whose starting point is nearest to the current segment's  out of all segment's starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a2d6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_higher_index(heading_index,start_indices):\n",
    "    sorted_indices =  sorted(start_indices)\n",
    "    end_index = sorted_indices.index(heading_index)+1\n",
    "    if end_index == len(sorted_indices):\n",
    "        end_index_val = len(cleantext)\n",
    "    else:\n",
    "        end_index_val =  sorted_indices[end_index]\n",
    "    return end_index_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77180ce2",
   "metadata": {},
   "source": [
    "For extracting sections like introduction, abstract we need to find its starting point and ending point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae2c3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_and_end_index(headings,start_indices):\n",
    "    heading_prob_points = []\n",
    "    for heading in available_headings:\n",
    "        \n",
    "        start_index_val = start_indices[available_headings.index(heading)]\n",
    "        end_index_val =nearest_higher_index(start_index_val,start_indices)\n",
    "        print((heading,start_index_val,end_index_val))\n",
    "        heading_prob_points.append((heading,start_index_val,end_index_val))\n",
    "    return heading_prob_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db484e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('introduction', 1249, 5584)\n",
      "('abstract', 330, 1249)\n",
      "('references', 20127, 23531)\n",
      "('proposed system', 5584, 19391)\n",
      "('conclusion and future work', 19391, 20127)\n"
     ]
    }
   ],
   "source": [
    "heading_prob_points= start_and_end_index(headings,start_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c86c9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_headings = [\"introduction\",\"abstract\",\"conclusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afc71ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTRODUCTION\n",
      "\n",
      "the arabic language is a highly in\u0003ected natural\n",
      "language that has an enormous number of possi-\n",
      "ble words (othman et al., 2003). and although it\n",
      "is the native language of over 300 million people,\n",
      "it suffers from the lack of useful resources as op-\n",
      "posed to other languages, specially english and\n",
      "until now there are no systems that cover the wide\n",
      "range of possible spelling errors. fortunately the\n",
      "qalb corpus (zaghouani et al., 2014) will help\n",
      "enrich the resources for arabic language generally\n",
      "and the spelling correction speci\u0002cally by provid-\n",
      "ing an annotated corpus with corrected sentences\n",
      "from user comments, native student essays, non-\n",
      "native data and machine translation data. in this\n",
      "work, we are trying to use this corpus to build an\n",
      "error correction system that can cover a range of\n",
      "spelling errors.\n",
      "this paper is a system description paper that is\n",
      "submitted in the emnlp 2014 conference shared\n",
      "task ﬂautomatic arabic error correctionﬂ (mohit\n",
      "et al., 2014) in the arabic nlp workshop. the\n",
      "challenges that faced us while working on this sys-\n",
      "tem was the shortage of contribution in the area\n",
      "of spelling correction in the arabic language. but\n",
      "hopefully the papers and the work in this shared\n",
      "task speci\u0002cally and in the workshop generally\n",
      "will enrich this area and \u0003ourish it.\n",
      "our system targets four types of spelling errors,\n",
      "edit errors, add before errors, merge errors and\n",
      "split errors. for each error type, a model is built\n",
      "to correct erroneous words detected by the error\n",
      "detection technique. edit errors and add before\n",
      "errors are corrected using classi\u0002ers with contex-\n",
      "tual features, while the merge and split errors are\n",
      "corrected by inserting or omitting a space between\n",
      "words and choosing the best candidate based on\n",
      "the language model score of each candidate.\n",
      "the rest of this paper is structured as follows.\n",
      "in section 2, we give a brief background on re-\n",
      "lated work in spelling correction. in section 3, we\n",
      "introduce our system for spelling correction with\n",
      "the description of the ef\u0002cient models used in the\n",
      "system. in section 4, we list some experimental re-\n",
      "sults on the development set. in section 5, we give\n",
      "some concluding remarks.\n",
      "2 related work\n",
      "the work in the \u0002eld of spelling correction in the\n",
      "arabic language is not yet mature and no sys-\n",
      "tem achieved a great error correction ef\u0002ciency.\n",
      "even microsoft word, the most widely used ara-\n",
      "bic spelling correction system, does not achieve\n",
      "good results. our work was inspired by a num-\n",
      "ber of papers. (shaalan et al., 2012) addressed\n",
      "the problem of arabic word generation for spell\n",
      "checking and they produced an open source and\n",
      "large coverage word list for arabic containing 9\n",
      "million fully in\u0003ected surface words and applied\n",
      "language models and noisy channel model and\n",
      "knowledge-based rules for error correction. this\n",
      "word list is used in our work besides using lan-\n",
      "guage models and noisy channel model.\n",
      "(shaalan et al., 2010) proposed another sys-\n",
      "tem for cases in which the candidate genera-\n",
      "tion using edit algorithm only was not enough,\n",
      "in which candidates were generated based on\n",
      "transformation rules and errors are detected using\n",
      "bama (buckwalter arabic morphological ana-\n",
      "lyzer)(buckwalter, 2002).\n",
      "(khalifa et al., 2011) proposed a system for text\n",
      "segmentation. the system discriminates between\n",
      "waw wasl and waw fasl, and depending on this\n",
      "it can predict if the sentence to be segmented at\n",
      "this position or not, they claim that they achieved\n",
      "97.95% accuracy. the features used in this work\n",
      "inspired us with the add before errors correction.\n",
      "(schaback, 2007) proposed a system for the en-\n",
      "glish spelling correction, that is addressing the edit\n",
      "errors on various levels: on the phonetic level us-\n",
      "ing\n",
      "soundex\n",
      "algorithm, on the character level us-\n",
      "ing edit algorithm with one operation away, on the\n",
      "word level using bigram language model, on the\n",
      "syntactic level using collocation model to deter-\n",
      "mine how \u0002t the candidate is in this position and\n",
      "on the semantic level using co-occurrence model\n",
      "to determine how likely a candidate occurs within\n",
      "the given context, using all the models output of\n",
      "candidate word as features and using svm model\n",
      "to classify the candidates, they claim reaching re-\n",
      "call ranging from 90% for \u0002rst candidate and 97%\n",
      "for all \u0002ve candidates presented and outperform-\n",
      "ing ms word, aspell, hunspell, fst and google.\n",
      "3 \n",
      "ABSTRACT\n",
      "\n",
      "in this work, we address the problem\n",
      "of spelling correction in the arabic lan-\n",
      "guage utilizing the new corpus provided\n",
      "by qalb (qatar arabic language bank)\n",
      "project which is an annotated corpus of\n",
      "sentences with errors and their corrections.\n",
      "the corpus contains edit, add before, split,\n",
      "merge, add after, move and other error\n",
      "types. we are concerned with the \u0002rst four\n",
      "error types as they contribute more than\n",
      "90% of the spelling errors in the corpus.\n",
      "the proposed system has many models to\n",
      "address each error type on its own and then\n",
      "integrating all the models to provide an\n",
      "ef\u0002cient and robust system that achieves\n",
      "an overall recall of 0.59, precision of 0.58\n",
      "and f1 score of 0.58 including all the error\n",
      "types on the development set. our system\n",
      "participated in the qalb 2014 shared task\n",
      "ﬂautomatic arabic error correctionﬂ and\n",
      "achieved an f1 score of 0.6, earning the\n",
      "sixth place out of nine participants.\n",
      "1 \n"
     ]
    }
   ],
   "source": [
    "for heading in available_headings:\n",
    "    index = available_headings.index(heading)\n",
    "    prob_points = heading_prob_points[index]\n",
    "    text = cleantext[prob_points[1]+len(heading):prob_points[2]]\n",
    "    if heading in common_headings:\n",
    "        print(heading.upper())\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a2cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
