{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa74bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbaf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"bbctech.txt\", \"r\")\n",
    "tech_data= f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d563067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    clean_text = [word.lower() for word in text.translate(translator).split()]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6ff4b",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "\n",
    "Tokenization is the process of  splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.The tokens can  be  words, numbers or punctuation marks. In tokenization, smaller units are created by locating word boundaries.If we do tokenization at paragraph level, then sentences will be tokens. If tokenization is done at sentence level, tokens will be words.\n",
    "\n",
    "Methods of tokenization\n",
    "* Tokenization using Regular Expressions (RegEx) \n",
    "* Tokenization using NLTK\n",
    "    * Sentence Tokenization\n",
    "    * Word Tokenization\n",
    "* Tokenization using **split** function\n",
    "\n",
    "Beside these methods there are other ways of tokenization like tokenization using keras, spacy etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ee72b",
   "metadata": {},
   "source": [
    "## Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e921ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.',\n",
       " 'Its a part of data preprocessing']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "  \n",
    "text = \"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. Its a part of data preprocessing\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6e68062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8167c",
   "metadata": {},
   "source": [
    "# Lemmatization \n",
    "\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization  brings context to the words. So it links words with similar meanings to one word.\n",
    "For example:  run is lemma for all words runs, running, ran .\n",
    "\n",
    "Sample text is taken from [Kaggle](https://www.kaggle.com/datasets/shivamkushwaha/bbc-full-text-document-classification?resource=download) entitled as BBC Full Text Document Classification . WordNetLemmatizer , available in NLTK (Natural Language Toolkit) is used for lemmatization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "138e0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import lemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentences = tech_data.splitlines()\n",
    "for sentence in sentences:\n",
    "    clean_text = text_preprocessing(sentence)\n",
    "    sent_lemma = ' '.join([lemmatizer.lemmatize(word) for word in clean_text])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c21449",
   "metadata": {},
   "source": [
    "# Parts of Speech(POS)\n",
    "\n",
    "\n",
    "POS tagging is the process of converting a sentence  list of tuples where each tuple is  in the form of (word, tag)). The tag signifies whether the word is a noun, verb, adjective and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ca71a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('travelling', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('mountain', 'NN'),\n",
       " ('areas', 'NNS')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I love travelling in mountain areas\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e1594",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "\n",
    "It is the process of extraction of short phrases from the text. It works on top of POS tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "984a2945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'travelling', 'in', 'mountain', 'areas', 'and', 'this', 'can', 'help', 'in', 'promoting', 'internal', 'tourism']\n",
      "[('I', 'PRP'), ('love', 'VBP'), ('travelling', 'VBG'), ('in', 'IN'), ('mountain', 'NN'), ('areas', 'NNS'), ('and', 'CC'), ('this', 'DT'), ('can', 'MD'), ('help', 'VB'), ('in', 'IN'), ('promoting', 'VBG'), ('internal', 'JJ'), ('tourism', 'NN')]\n",
      "(S\n",
      "  I/PRP\n",
      "  love/VBP\n",
      "  travelling/VBG\n",
      "  in/IN\n",
      "  (chunk mountain/NN areas/NNS and/CC)\n",
      "  this/DT\n",
      "  can/MD\n",
      "  help/VB\n",
      "  in/IN\n",
      "  promoting/VBG\n",
      "  internal/JJ\n",
      "  (chunk tourism/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"I love travelling in mountain areas and this can help in promoting internal tourism\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "chunkGram = \"\"\"chunk:{<NN.?>*<VBD.?>*<CC>?}\"\"\"\n",
    "chunkParser  =nltk.RegexpParser(chunkGram)\n",
    "result = chunkParser.parse(tag)\n",
    "print(result)\n",
    "\n",
    "result.draw()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7146c",
   "metadata": {},
   "source": [
    "# Chinking\n",
    "\n",
    "\n",
    "Its  a way for to remove a chunk from a chunk. The chunk that we remove from our chunk is termed chink.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8bb3f638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'travelling', 'in', 'mountain', 'areas', 'and', 'this', 'can', 'help', 'in', 'promoting', 'internal', 'tourism']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "\n",
    "chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tag)\n",
    "\n",
    "chunked.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
